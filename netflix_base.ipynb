{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfgO6blwzC3a"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNlPR54i0-u4"
      },
      "source": [
        "### Installazione e import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvrI5cdT0xqz"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.dataframe import DataFrame\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    last,\n",
        "    lit,\n",
        "    split,\n",
        "    regexp_extract,\n",
        "    monotonically_increasing_id,\n",
        "    count,\n",
        "    when,\n",
        "    concat_ws,\n",
        "    min as spark_min,\n",
        "    datediff,\n",
        "    to_date,\n",
        "    avg,\n",
        ")\n",
        "from pyspark.ml.recommendation import ALS, ALSModel\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se la variabile JAVA_HOME non è impostata globalmente, scommentare la cella sotto per popolarla, dopo aver inserito il percorso alla JDK di propria scelta nel file [.env](./.env)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load_dotenv()\n",
        "#!echo $JAVA_HOME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFxX1k5BytAN",
        "outputId": "c82bc5a9-5215-4b98-dcc1-0a457676ca78"
      },
      "outputs": [],
      "source": [
        "!pyspark --version\n",
        "!which python\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPIg0MsrC4MR"
      },
      "outputs": [],
      "source": [
        "class Costanti:\n",
        "    BASE_PATH = \"./resources/dataset/archive\"\n",
        "    SALVATAGGI_PATH = \"./resources/salvataggi\"\n",
        "    USA_SALVATAGGI = True\n",
        "    DF_RATING_OTTIMIZZATO = \"df_ratings_ottimizzato.parquet\"\n",
        "    DF_COMPLETO_BIAS = \"Tutti_con_bias\"\n",
        "    DF_CAMPIONE_BIAS = \"Campione_con_bias\"\n",
        "    DF_COMPLETO = \"Tutti\"\n",
        "    DF_CAMPIONE = \"Campione\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glt-YhaO1EXR"
      },
      "source": [
        "### Caricamento dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGkKvdmJ1IjK"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    spark.stop()\n",
        "except: pass\n",
        "try:\n",
        "    sc.stop()\n",
        "except: pass\n",
        "\n",
        "# Memoria impostata a 6 GB + 6 GB per rientrare nei 12.7 GB di Colab\n",
        "# Può essere personalizzata in base alle esigenze del proprio ambiente di esecuzione\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Netflix Recommendation System\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"1000\") \\\n",
        "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"MovieID\", IntegerType(), True),\n",
        "    StructField(\"CustomerID\", IntegerType(), True),\n",
        "    StructField(\"Rating\", IntegerType(), True),\n",
        "    StructField(\"Date\", StringType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Pt_fxQ1Xq7"
      },
      "outputs": [],
      "source": [
        "def persist_df(df, filename, path):\n",
        "    \"\"\"\n",
        "    Salva il DataFrame pyspark in formato parquet\n",
        "\n",
        "    Parametri:\n",
        "        df (DataFrame): il DataFrame da salvare\n",
        "        filename (str): il nome del file\n",
        "        path (str): il percorso del dataframe da salvare\n",
        "\n",
        "    Ritorna:\n",
        "        None\n",
        "    \"\"\"\n",
        "    df.write.mode(\"overwrite\").parquet(f\"{path}/{filename}\")\n",
        "\n",
        "def load_df(filename):\n",
        "    \"\"\"\n",
        "    Carica il DataFrame pyspark da un file in formato parquet\n",
        "\n",
        "    Parametri:\n",
        "        filename (str): il nome del file\n",
        "\n",
        "    Ritorna:\n",
        "        il DataFrame pyspark in formato parquet\n",
        "    \"\"\"\n",
        "    return spark.read.parquet(f\"{Costanti.SALVATAGGI_PATH}/{filename}\")\n",
        "\n",
        "def checkFile(path, filename):\n",
        "    \"\"\"\n",
        "    Verifica se il file esitse\n",
        "\n",
        "    Parametri:\n",
        "        path (str): il percorso del file\n",
        "        filename (str): il nome del file\n",
        "\n",
        "    Ritorna:\n",
        "        True se il file esiste, False altrimenti\n",
        "    \"\"\"\n",
        "    return os.path.exists(os.path.join(path, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (not checkFile(Costanti.BASE_PATH, '*')):\n",
        "    with ZipFile(\"./resources/dataset/archive.zip\", mode=\"r\") as archive:\n",
        "        archive.extractall(\"./resources/dataset/archive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrX6G77C2OS6"
      },
      "source": [
        "### Utilizzo dei file parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FidBmFf32Qs6",
        "outputId": "46c95933-af57-46f6-9c17-aee048845518"
      },
      "outputs": [],
      "source": [
        "percorso_salvataggio_parquet = os.path.join(Costanti.SALVATAGGI_PATH, Costanti.DF_RATING_OTTIMIZZATO)\n",
        "\n",
        "if Costanti.USA_SALVATAGGI and checkFile(Costanti.SALVATAGGI_PATH, Costanti.DF_RATING_OTTIMIZZATO):\n",
        "    print(f\" Caricamento dal file Parquet esistente: {percorso_salvataggio_parquet}\")\n",
        "    df_ratings = spark.read.parquet(percorso_salvataggio_parquet)\n",
        "else:\n",
        "    print(\"File Parquet non trovato. Salvataggio del file parquet\")\n",
        "    file_paths = [os.path.join(Costanti.BASE_PATH, f\"combined_data_{i}.txt\") for i in range(1, 5)]\n",
        "    df_raw = spark.read.text(file_paths)\n",
        "\n",
        "    df_ordered = df_raw.withColumn(\"order_id\", monotonically_increasing_id())\n",
        "\n",
        "    # Uso delle regex per trovare l'id del film\n",
        "    df_with_id = df_ordered.withColumn(\n",
        "        \"MovieID_temp\",\n",
        "        regexp_extract(col(\"value\"), r\"(\\d+):\", 1).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "    window_spec = Window.orderBy(\"order_id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "    df_filled = df_with_id.withColumn(\n",
        "        \"MovieID\",\n",
        "        last(col(\"MovieID_temp\"), ignorenulls=True).over(window_spec)\n",
        "    )\n",
        "\n",
        "    # Filtra le righe che non contengono \":\" e crea colonne strutturate splittando sulla \",\"\n",
        "    df_parsed = df_filled.filter(~col(\"value\").contains(\":\")) \\\n",
        "        .withColumn(\"data\", split(col(\"value\"), \",\")) \\\n",
        "        .select(\n",
        "            col(\"MovieID\"),\n",
        "            col(\"data\").getItem(0).cast(\"integer\").alias(\"CustomerID\"),\n",
        "            col(\"data\").getItem(1).cast(\"integer\").alias(\"Rating\"),\n",
        "            col(\"data\").getItem(2).cast(\"string\").alias(\"Date\")\n",
        "        )\n",
        "    df_ratings = df_parsed.cache()\n",
        "\n",
        "    print(f\" Trasformazione completata in formato parquet. Righe totali: {df_ratings.count()}\")\n",
        "\n",
        "    print(f\" Salvataggio del DataFrame in formato Parquet su: {percorso_salvataggio_parquet}\")\n",
        "    # vedere se da cambiare\n",
        "    df_ratings.write.mode(\"overwrite\").parquet(percorso_salvataggio_parquet)\n",
        "\n",
        "df_ratings.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzoKQ8oYifsh",
        "outputId": "36f33928-f9e6-477b-c2de-3bfb61b1f1e9"
      },
      "outputs": [],
      "source": [
        "# Nuovo percorso di salvataggio per il test set completo\n",
        "percorso_probe_parquet = os.path.join(Costanti.SALVATAGGI_PATH, \"df_probe_test_set.parquet\")\n",
        "percorso_probe_parquet_keys = os.path.join(Costanti.SALVATAGGI_PATH, \"df_probe_keys.parquet\")\n",
        "\n",
        "# Controlliamo se il test set completo esiste già\n",
        "if Costanti.USA_SALVATAGGI and (os.path.exists(percorso_probe_parquet) and os.path.exists(percorso_probe_parquet_keys)):\n",
        "    print(f\"Caricamento del test set completo dal file Parquet: {percorso_probe_parquet}\")\n",
        "    df_probe_test_set = spark.read.parquet(percorso_probe_parquet)\n",
        "    df_probe_keys = spark.read.parquet(percorso_probe_parquet_keys)\n",
        "else:\n",
        "    print(\"File Parquet del test set non trovato. Lo ricostruiamo...\")\n",
        "    print(\"FASE A: Estrazione delle chiavi (MovieID, CustomerID) da probe.txt...\")\n",
        "\n",
        "    file_path = os.path.join(Costanti.BASE_PATH, \"probe.txt\")\n",
        "    df_raw = spark.read.text(file_path)\n",
        "\n",
        "    df_ordered = df_raw.withColumn(\"order_id\", monotonically_increasing_id())\n",
        "\n",
        "    df_with_id = df_ordered.withColumn(\n",
        "        \"MovieID_temp\",\n",
        "        regexp_extract(col(\"value\"), r\"(\\d+):\", 1).cast(\"integer\")\n",
        "    )\n",
        "\n",
        "    window_spec = Window.orderBy(\"order_id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "    df_filled = df_with_id.withColumn(\"MovieID\", last(col(\"MovieID_temp\"), ignorenulls=True).over(window_spec))\n",
        "\n",
        "    df_customers = df_filled.filter(~col(\"value\").endswith(\":\"))\n",
        "\n",
        "    # Questo DataFrame di probe contiene solo le chiavi, senza i ratings\n",
        "    df_probe_keys = df_customers.withColumn(\"CustomerID\", col(\"value\").cast(\"integer\")) \\\n",
        "                                .select(\"MovieID\", \"CustomerID\")\n",
        "\n",
        "    print(\"Chiavi del probe set estratte con successo :)\")\n",
        "\n",
        "    df_probe_test_set = df_ratings.join(\n",
        "        df_probe_keys,\n",
        "        on=[\"CustomerID\", \"MovieID\"],\n",
        "        how=\"inner\"\n",
        "    )\n",
        "\n",
        "    df_probe_test_set.write.mode(\"overwrite\").parquet(percorso_probe_parquet)\n",
        "    df_probe_keys.write.mode(\"overwrite\").parquet(percorso_probe_parquet_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq6wkcEkl2jP",
        "outputId": "ccc338b1-8965-4285-8eae-50c9a52caebf"
      },
      "outputs": [],
      "source": [
        "df_ratings = df_ratings.join(\n",
        "  df_probe_keys,\n",
        "  on=[\"CustomerID\", \"MovieID\"],\n",
        "  how=\"left_anti\"\n",
        ")\n",
        "\n",
        "print(\"\\nAnteprima del test set finale (df_probe_test_set):\")\n",
        "df_probe_test_set.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk99QPqr2Res"
      },
      "outputs": [],
      "source": [
        "# shutil.rmtree(\"./salvataggi/df_ratings\", ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfL4CJzn2a04",
        "outputId": "5e04c7e5-736e-4639-950c-c2eed9f240a9"
      },
      "outputs": [],
      "source": [
        "#BASE_PATH = \"/content/dataset/archive\"\n",
        "\n",
        "schema_movies = StructType([\n",
        "    StructField(\"MovieId\", IntegerType(), True),\n",
        "    StructField(\"Year\", IntegerType(), True),\n",
        "    StructField(\"Name\", StringType(), True)\n",
        "])\n",
        "\n",
        "df_movies = (\n",
        "    spark.read.format(\"csv\")\n",
        "        .option(\"header\", \"false\")\n",
        "        .schema(schema_movies)\n",
        "        .load(f\"{Costanti.BASE_PATH}/movie_titles.csv\")\n",
        ")\n",
        "df_movies = df_movies.withColumnRenamed('MovieId', 'MovieID')\n",
        "\n",
        "df_movies.show()\n",
        "\n",
        "print(\"Ci sono\", df_movies.count(), \"film\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0AMDi1r23Iz"
      },
      "outputs": [],
      "source": [
        "# if not os.path.exists(\"./salvataggi/df_ratings_ottimizzato.parquet\") or not os.path.exists(\"./salvataggi/df_ratings\"):\n",
        "#     persist_df(df_ratings, \"df_ratings\", \"./salvataggi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F89zfWm3e8U"
      },
      "source": [
        "# Addestramento sistema di raccomandazione (Alternating Least Squares)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vq_dP8G39av"
      },
      "source": [
        "### Utilizzo dei file parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUXGFgFFh_UZ"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"StratifiedSampling\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydFBet12iCNL"
      },
      "outputs": [],
      "source": [
        "df_per_campionamento = df_ratings\n",
        "user_counts = df_per_campionamento.groupBy('CustomerID').agg(count('*').alias('user_rating_count'))\n",
        "movie_counts = df_per_campionamento.groupBy('MovieID').agg(count('*').alias('movie_rating_count'))\n",
        "\n",
        "df_per_campionamento = df_per_campionamento.join(user_counts, 'CustomerID').join(movie_counts, 'MovieID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jprMhTsEiK_h"
      },
      "outputs": [],
      "source": [
        "# Definizione degli strati\n",
        "df_per_campionamento = df_per_campionamento.withColumn('strato_utente',\n",
        "                       when(col('user_rating_count') <= 10, 'utente_poco_attivo')\n",
        "                       .when(col('user_rating_count') <= 30, 'utente_medio_attivo')\n",
        "                       .otherwise('utente_molto_attivo'))\n",
        "\n",
        "df_per_campionamento = df_per_campionamento.withColumn('strato_film',\n",
        "                       when(col('movie_rating_count') <= 20, 'film_nicchia')\n",
        "                       .when(col('movie_rating_count') <= 60, 'film_popolare')\n",
        "                       .otherwise('film_molto_popolare'))\n",
        "\n",
        "# Unione degli strati\n",
        "df_per_campionamento = df_per_campionamento.withColumn('strato_combinato', concat_ws('_', col('strato_utente'), col('strato_film')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQy2vRwUiU7L"
      },
      "outputs": [],
      "source": [
        "strati = df_per_campionamento.select('strato_combinato').distinct().collect()\n",
        "fractions = {row['strato_combinato']: 0.05 for row in strati} # Esempio: 5% da ogni strato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRe6WfO-3vmA",
        "outputId": "d2fe83c4-c9f7-4ef4-9e44-68242212cb6f"
      },
      "outputs": [],
      "source": [
        "df_campionato = df_per_campionamento.stat.sampleBy('strato_combinato', fractions=fractions, seed=42)\n",
        "\n",
        "print(f\"Dimensione del DataFrame originale 'df_ratings': {df_ratings.count()}\")\n",
        "print(f\"Dimensione del campione stratificato 'df_campionato': {df_campionato.count()}\")\n",
        "\n",
        "print(\"Distribuzione del campione stratificato:\")\n",
        "df_campionato.groupBy('strato_combinato').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5waqKwN4wIj"
      },
      "source": [
        "### Definizione del modello con ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gklRcyxeFo3r"
      },
      "outputs": [],
      "source": [
        "als_on_residuals = ALS(\n",
        "        maxIter=10, regParam=0.1,\n",
        "        userCol=\"CustomerID\", itemCol=\"MovieID\", ratingCol=\"residual\",\n",
        "        rank=10, coldStartStrategy=\"drop\", nonnegative=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99oyHi7e0TP_"
      },
      "outputs": [],
      "source": [
        "als_on_ratings = ALS(\n",
        "        maxIter=10, regParam=0.1,\n",
        "        userCol=\"CustomerID\", itemCol=\"MovieID\", ratingCol=\"Rating\",\n",
        "        rank=10, coldStartStrategy=\"drop\", nonnegative=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nt0_WPu4vje"
      },
      "outputs": [],
      "source": [
        "def calcola_bias(df_input: DataFrame):\n",
        "    \"\"\"\n",
        "    Calcola la media globale (μ), il bias per item (b_i) e il bias per utente (b_x)\n",
        "    da un DataFrame di rating in input.\n",
        "\n",
        "    Parametri:\n",
        "    df_input (Dataframe): DataFrame che deve contenere le colonne 'Rating', 'MovieID', 'CustomerID'.\n",
        "\n",
        "    Ritorna:\n",
        "    Una tupla contenente (mu, item_biases_df, user_biases_df).\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Calcolo della media globale (μ)\")\n",
        "    mu = df_input.agg(avg(\"Rating\")).collect()[0][0]\n",
        "\n",
        "    print(\"Calcolo del bias per item (b_i)\")\n",
        "    item_biases_df = df_input.groupBy(\"MovieID\").agg((avg(col(\"Rating\")) - mu).alias(\"b_i\"))\n",
        "\n",
        "    df_with_item_bias = df_input.join(item_biases_df, \"MovieID\", \"left\")\n",
        "    print(\"Calcolo del bias per utente (b_x)...\")\n",
        "    user_biases_df = df_with_item_bias.groupBy(\"CustomerID\").agg(\n",
        "        (avg(col(\"Rating\") - lit(mu) - col(\"b_i\"))).alias(\"b_x\")\n",
        "    )\n",
        "\n",
        "    print(f\"Calcolo dei bias completato. Valore della media: {mu:.4f}\")\n",
        "\n",
        "    return mu, item_biases_df, user_biases_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoUZUL_H46eQ"
      },
      "source": [
        "### Produzione grafici di BellKor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC8IwZ2HL9Ex",
        "outputId": "f08f33b7-9558-449d-c0f4-0460e4468f68"
      },
      "outputs": [],
      "source": [
        "(mu, item_biases_df, user_biases_df) = calcola_bias(df_per_campionamento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFKCwtZfmc6M"
      },
      "outputs": [],
      "source": [
        "def fit_model_with_bias(df_to_use, nome):\n",
        "    \"\"\"\n",
        "    Esegue la fit di un modello ALS con termini di bias utilizzando la tecnica BellKor\n",
        "\n",
        "    Parametri:\n",
        "        df_to_us (DataFrame): DataFrame di training con le colonne CustomerID, MovieID, Rating\n",
        "        nome (str): Nome del modello da usare\n",
        "\n",
        "    Ritorna:\n",
        "        DataFrame: Dati di training con i residuals\n",
        "\n",
        "    \"\"\"\n",
        "    df_to_use = df_to_use.dropna(subset=[\"Rating\"])\n",
        "    save_path = f\"{Costanti.SALVATAGGI_PATH}/modelli_als/{nome}_con_bias\"\n",
        "\n",
        "    df_with_biases = df_to_use \\\n",
        "        .join(item_biases_df, \"MovieID\", \"left\") \\\n",
        "        .join(user_biases_df, \"CustomerID\", \"left\")\n",
        "\n",
        "    # Calcoliamo il residuo e gestiamo i null\n",
        "\n",
        "    df_for_als_training = df_with_biases.withColumn(\n",
        "        \"residual\",\n",
        "        col(\"Rating\") - lit(mu) - col(\"b_i\") - col(\"b_x\")\n",
        "    ).fillna(0, subset=[\"residual\", \"b_i\", \"b_x\"])\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        print(f\"[{nome}] Addestramento\")\n",
        "        model = als_on_residuals.fit(df_for_als_training)\n",
        "        model.write().overwrite().save(save_path)\n",
        "    else:\n",
        "        print(f\"[{nome}] Caricamento del modello da {save_path}\")\n",
        "        model = ALSModel.load(save_path)\n",
        "\n",
        "    print(f\"[{nome}] Calcolo delle predizioni e RMSE\")\n",
        "    predictions_residual = model.transform(df_probe_test_set)\n",
        "\n",
        "    predictions_with_biases = predictions_residual \\\n",
        "        .join(item_biases_df, \"MovieID\", \"left\") \\\n",
        "        .join(user_biases_df, \"CustomerID\", \"left\")\n",
        "\n",
        "    final_predictions = predictions_with_biases.withColumn(\n",
        "        \"final_prediction\",\n",
        "        col(\"prediction\") + lit(mu) + col(\"b_i\") + col(\"b_x\")\n",
        "    )\n",
        "\n",
        "    evaluator = RegressionEvaluator(\n",
        "        metricName=\"rmse\", labelCol=\"Rating\", predictionCol=\"final_prediction\"\n",
        "    )\n",
        "    rmse = evaluator.evaluate(final_predictions)\n",
        "    print(f\"RMSE ({nome}): {rmse:.4f}\\n\")\n",
        "    return df_for_als_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwfXhHWxUKBO"
      },
      "outputs": [],
      "source": [
        "def fit_model_no_bias(df_to_use, nome):\n",
        "    \"\"\"\n",
        "    Addestra un modello ALS standard sui rating originali.\n",
        "\n",
        "    Parametri:\n",
        "    df_to_use (DataFrame):  DataFrame di training con le colonne CustomerID, MovieID, Rating\n",
        "\n",
        "    nome (str): Nome del modello da utilizzare\n",
        "\n",
        "    Ritorna:\n",
        "    DataFrame: Dataset di training utilizzato per l'addestramento\n",
        "    \"\"\"\n",
        "    df_to_use = df_to_use.dropna(subset=[\"Rating\"])\n",
        "    save_path = f\"{Costanti.SALVATAGGI_PATH}/modelli_als/{nome}\"\n",
        "\n",
        "    # Calcoliamo il residuo e gestiamo i null\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        print(f\"Addestramento del modello per '{nome}'...\")\n",
        "        model = als_on_ratings.fit(df_to_use)\n",
        "        model.write().overwrite().save(save_path)\n",
        "    else:\n",
        "        print(f\"Caricamento del modello per '{nome}' da {save_path}...\")\n",
        "        model = ALSModel.load(save_path)\n",
        "\n",
        "    predictions = model.transform(df_probe_test_set)\n",
        "    evaluator = RegressionEvaluator(\n",
        "        metricName=\"rmse\", labelCol=\"Rating\", predictionCol=\"prediction\"\n",
        "    )\n",
        "    rmse = evaluator.evaluate(predictions)\n",
        "    print(f\"RMSE ({nome}): {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrVf3h833_lK"
      },
      "outputs": [],
      "source": [
        "def grafici_bellKor(df_to_use, nome):\n",
        "    \"\"\"\n",
        "    Questa funzione applica i bias pre-calcolati, addestra un modello ALS sui residui,\n",
        "    calcola l'RMSE e genera i grafici di BellKor per il DataFrame fornito.\n",
        "\n",
        "    Parametri:\n",
        "    df_to_use (DataFrame): il dataFrame del quale vogliamo generare i grafici\n",
        "    nome (str): il nome del dataFrame del quale vogliamo generare i garfici\n",
        "\n",
        "    Ritorna:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"[{nome}] Generazione dei grafici di BellKor...\")\n",
        "\n",
        "    df_movies_cleaned = df_movies \\\n",
        "        .withColumn(\"Year\", col(\"Year\").cast(\"integer\")) \\\n",
        "        .withColumn(\"MovieID\", col(\"MovieID\").cast(\"integer\")) \\\n",
        "        .withColumn(\"ReleaseDate\", to_date(concat_ws(\"-\", col(\"Year\").cast(\"string\"), lit(\"01\"), lit(\"01\"))))\n",
        "\n",
        "    df_to_use = df_to_use.withColumn('Date', to_date(col('Date')))\n",
        "\n",
        "    # GRAFICO 1: Rating by date\n",
        "    w = Window.partitionBy()\n",
        "    df_with_days = df_to_use.withColumn('days_from_start', datediff(col('Date'), spark_min('Date').over(w)))\n",
        "    grouped_by_day_pd = df_with_days.groupBy('days_from_start').agg(avg('Rating').alias('mean_rating')).orderBy('days_from_start').toPandas()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.scatter(grouped_by_day_pd['days_from_start'], grouped_by_day_pd['mean_rating'], color='red', alpha=0.7, s=8)\n",
        "    plt.title(f'Rating by date ({nome})')\n",
        "    plt.xlabel('time (days)')\n",
        "    plt.ylabel('mean score')\n",
        "    plt.ylim(3.2, 3.9)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # GRAFICO 2: Rating by movie age\n",
        "    df_joined = df_to_use.join(df_movies_cleaned.select('MovieID', 'ReleaseDate'), on='MovieID', how='inner')\n",
        "    df_valid = df_joined.withColumn('movie_age_days', datediff(col('Date'), col('ReleaseDate'))).filter(col('movie_age_days').isNotNull() & (col('movie_age_days') >= 0))\n",
        "    grouped_by_age_pd = df_valid.groupBy('movie_age_days').agg(avg('Rating').alias('mean_rating')).orderBy('movie_age_days').toPandas()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.scatter(grouped_by_age_pd['movie_age_days'], grouped_by_age_pd['mean_rating'], color='red', alpha=0.7, s=8)\n",
        "    plt.title(f'Rating by movie age ({nome})')\n",
        "    plt.xlabel('movie age (days)')\n",
        "    plt.ylabel('mean score')\n",
        "    plt.ylim(3.2, 3.9)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"--- Fine analisi per '{nome}' ---\")\n",
        "    return df_to_use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81iku6zIpxpL"
      },
      "source": [
        "### Grafici di BellKor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpzlOXGDp3Sj"
      },
      "outputs": [],
      "source": [
        "grafici_bellKor(df_per_campionamento, 'Tutti')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mukWBGIIp5UT"
      },
      "outputs": [],
      "source": [
        "grafici_bellKor(df_campionato, 'Campione')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etOf1PL_1MwK"
      },
      "source": [
        "### Addestramento con bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsxkobOY1qFX"
      },
      "source": [
        "Dataset completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2HFjYT1l1R9A",
        "outputId": "8aeff910-dd34-48e4-c6a2-d595b561cadc"
      },
      "outputs": [],
      "source": [
        "df_biased = fit_model_with_bias(df_per_campionamento, 'Tutti')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrdeS3dn2KGG"
      },
      "source": [
        "Dataset campionato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzk1dxgV2RtL",
        "outputId": "2d96233c-b471-4512-9f39-489b5abd5acc"
      },
      "outputs": [],
      "source": [
        "df_biased = fit_model_with_bias(df_campionato, 'Campione')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3zT1VcE22LX"
      },
      "source": [
        "### Addestramento senza bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9N00Hkr2-ok"
      },
      "source": [
        "Dataset completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vwAL96_w27X5",
        "outputId": "4c047021-f803-496a-d7cf-a7ce5b39313f"
      },
      "outputs": [],
      "source": [
        "fit_model_no_bias(df_per_campionamento, 'Tutti')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRb8DY57274n"
      },
      "source": [
        "Dataset campionato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaVEaGvJ3Lmb"
      },
      "outputs": [],
      "source": [
        "fit_model_no_bias(df_campionato, 'Campione')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPcofdPU5M-4"
      },
      "source": [
        "# Raccomandazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD-sKp3y5HAe"
      },
      "outputs": [],
      "source": [
        "while(True):\n",
        "  try:\n",
        "    scelta = int(input(\"\"\"Su quale DataFrame effettuare la raccomandazione?\n",
        "                  1. Completo con bias\n",
        "                  2. Campione con bias\n",
        "                  3. Completo senza bias\n",
        "                  4. Campione senza bias\n",
        "                  > \n",
        "                  \"\"\"))\n",
        "  except ValueError:\n",
        "    print(\"Inserire un valore numerico.\")\n",
        "  \n",
        "  if scelta == 1:\n",
        "    dataframe = Costanti.DF_COMPLETO_BIAS\n",
        "    break\n",
        "  elif scelta == 2:\n",
        "    dataframe = Costanti.DF_CAMPIONE_BIAS\n",
        "    break\n",
        "  elif scelta == 3:\n",
        "    dataframe = Costanti.DF_COMPLETO\n",
        "    break\n",
        "  elif scelta == 4:\n",
        "    dataframe = Costanti.DF_CAMPIONE\n",
        "    break\n",
        "  else:\n",
        "    print(\"Effettuare una scelta valida.\")\n",
        "\n",
        "save_path = f\"{Costanti.SALVATAGGI_PATH}/modelli_als/{dataframe}\"\n",
        "\n",
        "print(f\"Tentativo di caricare il modello dal percorso: '{save_path}'\")\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "  print(\"Percorso trovato! Caricamento del modello...\")\n",
        "  model = ALSModel.load(save_path)\n",
        "\n",
        "  print(\"Modello caricato. Calcolo delle raccomandazioni...\")\n",
        "  model.recommendForAllUsers(10).show()\n",
        "else:\n",
        "  raise Exception(f\"Modello non trovato nel percorso: {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
